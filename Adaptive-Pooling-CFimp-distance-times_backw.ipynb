{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea inicial, arquitectura inicial de la CNN y usa carry-forward imputation, mas cantidad de mediciones en cada intervalo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os, time, random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:\n",
    "\n",
    "**Just run the next box to download the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this just the first time\n",
    "#!pip install -U wget\n",
    "#!rm -rf data.zip data lib\n",
    "#!rm -rf preprocessed\n",
    "!mkdir -p preprocessed\n",
    "!mkdir -p lib\n",
    "\n",
    "import wget\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/config.yaml', 'lib/config.yaml')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/helper.py', 'lib/helper.py')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/data.zip', 'data.zip')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/preprocessed/data_seq.npz', 'preprocessed/data_seq.npz')\n",
    "wget.download('https://github.com/shengpu1126/BDSI2019-ML/raw/master/lib/prepare_data.py', 'lib/prepare_data.py')\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files from disk: 100%|██████████| 12000/12000 [02:17<00:00, 87.55it/s] \n"
     ]
    }
   ],
   "source": [
    "#Run this to load the data if you have downloaded the data before\n",
    "from lib.helper import load_data\n",
    "raw_data, df_labels = load_data(120000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# GPU support\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damorgal/Documents/BDSI_project/lib/prepare_data_CF_dist-time_backw.py:9: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(open('lib/config.yaml'))\n",
      "Loading files from disk: 100%|██████████| 10000/10000 [01:55<00:00, 86.78it/s]\n",
      "Generating feature vectors: 100%|██████████| 10000/10000 [3:04:49<00:00,  1.11s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 48, 70) (10000,)\n"
     ]
    }
   ],
   "source": [
    "%run lib/prepare_data_CF_dist-time_backw.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor([self.y[idx]]).float()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "def get_train_val_test(batch_size=64):\n",
    "    #f = np.load('preprocessed/data_seq.npz')\n",
    "    f = np.load('data/data_nmiss_CF.npz')\n",
    "    f2 = np.load('data/data_miss_CF.npz')\n",
    "    f3 = np.load('data/data_dist_backw.npz')\n",
    "    f4 = np.load('data/data_times_backw_neg.npz')\n",
    "    X, y = f['X'], f['y']\n",
    "    X = np.concatenate((X,f2['X'],f3['X'],f4['X']), axis=2)\n",
    "    X = X.transpose((0,2,1))\n",
    "    print(X.shape, y.shape)\n",
    "    \n",
    "    print('Creating splits')\n",
    "    Xtr, X__, ytr, y__ = train_test_split(X,   y,   train_size=0.8, stratify=y,   random_state=1)\n",
    "    Xva, Xte, yva, yte = train_test_split(X__, y__, test_size=0.5, stratify=y__, random_state=1)\n",
    "    \n",
    "    tr = SimpleDataset(Xtr, ytr)\n",
    "    va = SimpleDataset(Xva, yva)\n",
    "    te = SimpleDataset(Xte, yte)\n",
    "    \n",
    "    tr_loader = DataLoader(tr, batch_size=batch_size, shuffle=True)\n",
    "    va_loader = DataLoader(va, batch_size=batch_size)\n",
    "    te_loader = DataLoader(te, batch_size=batch_size)\n",
    "    \n",
    "    print('Feature shape, Label shape, Class balance:')\n",
    "    print('\\t', tr_loader.dataset.X.shape, tr_loader.dataset.y.shape, tr_loader.dataset.y.mean())\n",
    "    print('\\t', va_loader.dataset.X.shape, va_loader.dataset.y.shape, va_loader.dataset.y.mean())\n",
    "    print('\\t', te_loader.dataset.X.shape, te_loader.dataset.y.shape, te_loader.dataset.y.mean())\n",
    "    return tr_loader, va_loader, te_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 140, 48) (10000,)\n",
      "Creating splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damorgal/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape, Label shape, Class balance:\n",
      "\t (8000, 140, 48) (8000,) 0.142875\n",
      "\t (1000, 140, 48) (1000,) 0.143\n",
      "\t (1000, 140, 48) (1000,) 0.143\n"
     ]
    }
   ],
   "source": [
    "tr_loader, va_loader, te_loader = get_train_val_test(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters, output_size, sequence_len):\n",
    "        super().__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.conv1 = nn.Conv1d(in_channels, n_filters, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(n_filters, n_filters, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(int(sequence_len/2/2)*n_filters, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xm = 1 - x[:,35:70,:]\n",
    "        xd = x[:,70:105,:]\n",
    "        xt = x[:,105:,:]\n",
    "        x = x[:,:35,:]\n",
    "        N, d, L = x.shape\n",
    "        x = torch.Tensor(np.concatenate((x,xd,xt), axis=1))\n",
    "        ones = np.array([np.array([np.ones(L) for i in range(d)]) for j in range(N)])\n",
    "        xm = torch.Tensor(np.concatenate((xm,ones,ones), axis=1))\n",
    "        \n",
    "        # Getting the weights for the trained model\n",
    "        z = self.conv1(x)\n",
    "        w = self.conv1.weight\n",
    "        wm = torch.abs(w) / torch.sum(torch.abs(w), dim=(1,2), keepdim=True)\n",
    "        \n",
    "        # Apply the weights to the mask xm\n",
    "        zm = F.conv1d(xm, wm, padding=1)\n",
    "        \n",
    "        # Apply the first set of conv-elu-pool\n",
    "        z = self.pool1(zm*F.elu(z))\n",
    "        \n",
    "        # Apply the second set of conv-elu-pool\n",
    "        z = self.pool2(F.elu(self.conv2(z)))\n",
    "        \n",
    "        # Flatten the output from the convolutional/pooling layers\n",
    "        z = z.view(N, -1)\n",
    "        z = self.fc(z)\n",
    "        #z = torch.stack(self.fc(z), 1)\n",
    "        \n",
    "        # Pass through the output layer and apply sigmoid activation\n",
    "        z = torch.sigmoid(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of float-valued parameters: 33345\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = CNN(35*3, 64, 1, 48)\n",
    "print('Number of float-valued parameters:', count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5007]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((1, 35*4, 48))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(data_loader, model, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Train the `model` for one epoch of data from `data_loader`\n",
    "    Use `optimizer` to optimize the specified `criterion`\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # clear parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def _evaluate_epoch(tr_loader, va_loader, model, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Evaluate on train\n",
    "        y_true, y_score = [], []\n",
    "        running_loss = []\n",
    "        for X, y in tr_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X)\n",
    "            y_true.append(y.cpu().numpy())\n",
    "            y_score.append(output.cpu().numpy())\n",
    "            running_loss.append(criterion(output, y).item())\n",
    "\n",
    "        y_true, y_score = np.concatenate(y_true), np.concatenate(y_score)\n",
    "        train_loss = np.mean(running_loss)\n",
    "        train_score = metrics.roc_auc_score(y_true, y_score)\n",
    "        print('tr loss', train_loss, 'tr AUROC', train_score)\n",
    "\n",
    "        # Evaluate on validation\n",
    "        y_true, y_score = [], []\n",
    "        running_loss = []\n",
    "        for X, y in va_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(X)\n",
    "                y_true.append(y.cpu().numpy())\n",
    "                y_score.append(output.cpu().numpy())\n",
    "                running_loss.append(criterion(output, y).item())\n",
    "\n",
    "        y_true, y_score = np.concatenate(y_true), np.concatenate(y_score)\n",
    "        val_loss = np.mean(running_loss)\n",
    "        val_score = metrics.roc_auc_score(y_true, y_score)\n",
    "        print('va loss', val_loss, 'va AUROC', val_score)\n",
    "    return train_loss, val_loss, train_score, val_score\n",
    "\n",
    "def save_checkpoint(model, epoch, checkpoint_dir):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "    }\n",
    "\n",
    "    filename = os.path.join(checkpoint_dir, 'epoch={}.checkpoint.pth.tar'.format(epoch))\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 140, 48) (10000,)\n",
      "Creating splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damorgal/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1b5a79b2c0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!mkdir -p checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtr_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_val_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9c04d8cbc740>\u001b[0m in \u001b[0;36mget_train_val_test\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating splits'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mXva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2212\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2212\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!mkdir -p checkpoint\n",
    "\n",
    "tr_loader, va_loader, te_loader = get_train_val_test(batch_size=64)\n",
    "    \n",
    "torch.random.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = CNN(35*3, 64, 1, 48)\n",
    "print('Number of float-valued parameters:', count_parameters(model))\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "print('Epoch', 0)\n",
    "out = _evaluate_epoch(tr_loader, va_loader, model, criterion)\n",
    "outputs.append(out)\n",
    "\n",
    "for epoch in range(0, n_epochs):\n",
    "    print('Epoch', epoch+1)\n",
    "    # Train model\n",
    "    _train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "    # Evaluate model\n",
    "    out = _evaluate_epoch(tr_loader, va_loader, model, criterion)\n",
    "    outputs.append(out)\n",
    "\n",
    "    # Save model parameters\n",
    "    save_checkpoint(model, epoch+1, 'checkpoint/')\n",
    "\n",
    "train_losses, val_losses, train_scores, val_scores = zip(*outputs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.plot(range(n_epochs + 1), train_scores, '--o', label='Train')\n",
    "plt.plot(range(n_epochs + 1), val_scores, '--o', label='Validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('AUROC')\n",
    "plt.legend()\n",
    "plt.savefig('auroc_dist_times_backw.png', dpi=300)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.plot(range(n_epochs + 1), train_losses, '--o', label='Train')\n",
    "plt.plot(range(n_epochs + 1), val_losses, '--o', label='Validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss (binary cross entropy)')\n",
    "plt.legend()\n",
    "plt.savefig('loss_dist_times_backw.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 140, 48) (10000,)\n",
      "Creating splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damorgal/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape, Label shape, Class balance:\n",
      "\t (8000, 140, 48) (8000,) 0.142875\n",
      "\t (1000, 140, 48) (1000,) 0.143\n",
      "\t (1000, 140, 48) (1000,) 0.143\n",
      "Which epoch to load from? Choose in range [1, 30].\n",
      "6\n",
      "Loading from checkpoint checkpoint/epoch=6.checkpoint.pth.tar\n",
      "=> Successfully restored checkpoint (trained for 6 epochs)\n",
      "Test loss : 0.3656015517190099\n",
      "Test AUROC: 0.7674763975814151\n"
     ]
    }
   ],
   "source": [
    "def restore_checkpoint(model, checkpoint_dir, cuda=False):\n",
    "    \"\"\"\n",
    "    If a checkpoint exists, restores the PyTorch model from the checkpoint.\n",
    "    Returns the model and the current epoch.\n",
    "    \"\"\"\n",
    "    cp_files = [file_ for file_ in os.listdir(checkpoint_dir)\n",
    "        if file_.startswith('epoch=') and file_.endswith('.checkpoint.pth.tar')]\n",
    "\n",
    "    if not cp_files:\n",
    "        print('No saved model parameters found')\n",
    "        if force:\n",
    "            raise Exception(\"Checkpoint not found\")\n",
    "        else:\n",
    "            return model, 0, []\n",
    "    \n",
    "    # Find latest epoch\n",
    "    for i in itertools.count(1):\n",
    "        if 'epoch={}.checkpoint.pth.tar'.format(i) in cp_files:\n",
    "            epoch = i\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"Which epoch to load from? Choose in range [1, {}].\".format(epoch))\n",
    "    inp_epoch = int(input())\n",
    "    if inp_epoch not in range(1, epoch+1):\n",
    "        raise Exception(\"Invalid epoch number\")\n",
    "\n",
    "    filename = os.path.join(checkpoint_dir,\n",
    "        'epoch={}.checkpoint.pth.tar'.format(inp_epoch))\n",
    "\n",
    "    print(\"Loading from checkpoint {}\".format(filename))\n",
    "    \n",
    "    if cuda:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(filename,\n",
    "            map_location=lambda storage, loc: storage)\n",
    "\n",
    "    try:\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\"=> Successfully restored checkpoint (trained for {} epochs)\"\n",
    "            .format(checkpoint['epoch']))\n",
    "    except:\n",
    "        print(\"=> Checkpoint not successfully restored\")\n",
    "        raise\n",
    "\n",
    "    return model, inp_epoch\n",
    "\n",
    "def _evaluate_epoch(data_loader, model, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true, y_score = [], []\n",
    "        running_loss = []\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            y_true.append(y.numpy())\n",
    "            y_score.append(output)\n",
    "            running_loss.append(criterion(output, y).item())\n",
    "        y_true, y_score = np.concatenate(y_true), np.concatenate(y_score)\n",
    "    \n",
    "    loss = np.mean(running_loss)\n",
    "    score = metrics.roc_auc_score(y_true, y_score)\n",
    "    return loss, score\n",
    "\n",
    "_, _, te_loader = get_train_val_test(batch_size=64)\n",
    "model = CNN(35*3, 64, 1, 48)\n",
    "model, _ = restore_checkpoint(model, 'checkpoint/')\n",
    "criterion = torch.nn.BCELoss()\n",
    "loss, score = _evaluate_epoch(te_loader, model, criterion)\n",
    "print('Test loss :', loss)\n",
    "print('Test AUROC:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
